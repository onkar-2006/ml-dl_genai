{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#installing depnedensis\n",
        "# %%capture\n",
        "# !pip install --upgrade transformers==4.41.2 sentence-transformers==3.0.1 gensim==4.3.2 scikit-learn==1.5.0 accelerate==0.31.0 peft==0.11.1 scipy==1.10.1 numpy==1.26.4"
      ],
      "metadata": {
        "id": "aA-VdyV25HF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "52PhMNaRDkiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=20\n",
        ")\n",
        "\n",
        "# Print the output\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "id": "OPoqGiRsDnw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaxh_PMqDqgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutomodelForcausalLm , AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )\n"
      ],
      "metadata": {
        "id": "BbwFo9hM9jZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµ é¸Ÿ\n",
        "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "b2R0XqKsDwfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ],
      "metadata": {
        "id": "Dvn0aQmX-aaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"gpt2\")"
      ],
      "metadata": {
        "id": "HrWRLF2Q-a9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"google/flan-t5-small\")"
      ],
      "metadata": {
        "id": "9FCZoH8s-d5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"Xenova/gpt-4\")"
      ],
      "metadata": {
        "id": "vMLbXccH-fw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"bigcode/starcoder2-15b\")"
      ],
      "metadata": {
        "id": "9zwnyeH9-gQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"facebook/galactica-1.3b\")"
      ],
      "metadata": {
        "id": "NAVdmD8E-jHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contexualize word embeddings"
      ],
      "metadata": {
        "id": "2tISHZft-o-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "FC21jZg2-oLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "# Load a language model\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer('Hello world', return_tensors='pt')\n",
        "\n",
        "# Process the tokens\n",
        "output = model(**tokens)[0]"
      ],
      "metadata": {
        "id": "3Mcj6feh_VFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "SQtNrREF_V8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))"
      ],
      "metadata": {
        "id": "XmsVZou6_YjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text embeddings"
      ],
      "metadata": {
        "id": "LLek33oO_h9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Convert text to text embeddings\n",
        "vector = model.encode(\"Best movie ever!\")"
      ],
      "metadata": {
        "id": "EHK8E69jD7IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word embeddings beyond llms"
      ],
      "metadata": {
        "id": "6Sa0KoWiAGEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import genssim.downloader import api"
      ],
      "metadata": {
        "id": "Ef7s1QRAAFdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=api.load('glove_wiki-gigaword-50')"
      ],
      "metadata": {
        "id": "NdNFBukRAQVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar([model['king']], topn=11)"
      ],
      "metadata": {
        "id": "JU6PN_ogAXuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## word embeddings-beyond_llms"
      ],
      "metadata": {
        "id": "rYhGGo3SEBxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n",
        "# Other options include \"word2vec-google-news-300\"\n",
        "# More options at https://github.com/RaRe-Technologies/gensim-data\n",
        "model = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "Lz45CELyEAEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SONG RECOMANDATION BY EMBEDDINGS"
      ],
      "metadata": {
        "id": "rXT728qyAnNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# Get the playlist dataset file\n",
        "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
        "\n",
        "# Parse the playlist dataset file. Skip the first two lines as\n",
        "# they only contain metadata\n",
        "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
        "\n",
        "# Remove playlists with only one song\n",
        "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
        "\n",
        "# Load song metadata\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
        "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
        "songs_df = songs_df.set_index('id')"
      ],
      "metadata": {
        "id": "HmdT_YhVAk8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( 'Playlist #1:\\n ', playlists[0], '\\n')\n",
        "print( 'Playlist #2:\\n ', playlists[1])"
      ],
      "metadata": {
        "id": "1Xg53pd-CHtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model=Word2Vec(\n",
        "    playlists ,vector_size , window=20 , negative=50 , min_count=1 workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "JnTQx-MJCZAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song_id = 2172\n",
        "# Ask the model for songs similar to song #2172\n",
        "model.wv.most_similar(positive=str(song_id))\n"
      ],
      "metadata": {
        "id": "teCy2ovZCsVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(songs_df.iloc[2172])"
      ],
      "metadata": {
        "id": "_5I2GBmiC2Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def print_recommandation(song_id):\n",
        "  similar_songs=np.array(\n",
        "      model.wv.most_similar(positive=str(song_id) , topn=5)\n",
        "  )[:,0]\n",
        "  return song_df.iloc[similar_songs]\n",
        "print_recommandation(2172)"
      ],
      "metadata": {
        "id": "6cWjWjsgC5BT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}