{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvaODoec8QtT"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from pickle import dump, load\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical, get_file\n",
        "from keras.layers import add\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "\n",
        "# small library for seeing the progress of loops.\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas()\n",
        "\n",
        "# Loading a text file into memory\n",
        "def load_doc(filename):\n",
        "    # Opening the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# get all imgs with their captions\n",
        "def all_img_captions(filename):\n",
        "    file = load_doc(filename)\n",
        "    captions = file.split('\\n')\n",
        "    descriptions ={}\n",
        "    for caption in captions[:-1]:\n",
        "        img, caption = caption.split('\\t')\n",
        "        if img[:-2] not in descriptions:\n",
        "            descriptions[img[:-2]] = [ caption ]\n",
        "        else:\n",
        "            descriptions[img[:-2]].append(caption)\n",
        "    return descriptions\n",
        "\n",
        "#Data cleaning- lower casing, removing puntuations and words containing numbers\n",
        "def cleaning_text(captions):\n",
        "    table = str.maketrans('','',string.punctuation)\n",
        "    for img,caps in captions.items():\n",
        "        for i,img_caption in enumerate(caps):\n",
        "\n",
        "            img_caption.replace(\"-\",\" \")\n",
        "            desc = img_caption.split()\n",
        "\n",
        "            #converts to lowercase\n",
        "            desc = [word.lower() for word in desc]\n",
        "            #remove punctuation from each token\n",
        "            desc = [word.translate(table) for word in desc]\n",
        "            #remove hanging 's and a\n",
        "            desc = [word for word in desc if(len(word)>1)]\n",
        "            #remove tokens with numbers in them\n",
        "            desc = [word for word in desc if(word.isalpha())]\n",
        "            #convert back to string\n",
        "\n",
        "            img_caption = ' '.join(desc)\n",
        "            captions[img][i]= img_caption\n",
        "    return captions\n",
        "\n",
        "def text_vocabulary(descriptions):\n",
        "    # build vocabulary of all unique words\n",
        "    vocab = set()\n",
        "\n",
        "    for key in descriptions.keys():\n",
        "        [vocab.update(d.split()) for d in descriptions[key]]\n",
        "\n",
        "    return vocab\n",
        "\n",
        "#All descriptions in one file\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + '\\t' + desc )\n",
        "    data = \"\\n\".join(lines)\n",
        "    file = open(filename,\"w\")\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "\n",
        "# Set these path according to project folder in you system\n",
        "dataset_text = \"/Users/sreemanti/Documents/youtube/youtube-teach/image caption generator/Flickr8k_text\"\n",
        "dataset_images = \"/Users/sreemanti/Documents/youtube/youtube-teach/image caption generator/Flicker8k_Dataset\"\n",
        "\n",
        "features = load(open(\"features.p\",\"rb\"))\n",
        "\n",
        "#load the data\n",
        "def load_photos(filename):\n",
        "    file = load_doc(filename)\n",
        "    photos = file.split(\"\\n\")[:-1]\n",
        "    photos_present = [photo for photo in photos if os.path.exists(os.path.join(dataset_images, photo))]\n",
        "    return photos_present\n",
        "\n",
        "\n",
        "def load_clean_descriptions(filename, photos):\n",
        "    #loading clean_descriptions\n",
        "    file = load_doc(filename)\n",
        "    descriptions = {}\n",
        "    for line in file.split(\"\\n\"):\n",
        "\n",
        "        words = line.split()\n",
        "        if len(words)<1 :\n",
        "            continue\n",
        "\n",
        "        image, image_caption = words[0], words[1:]\n",
        "\n",
        "        if image in photos:\n",
        "            if image not in descriptions:\n",
        "                descriptions[image] = []\n",
        "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
        "            descriptions[image].append(desc)\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "\n",
        "def load_features(photos):\n",
        "    #loading all features\n",
        "    all_features = load(open(\"features.p\",\"rb\"))\n",
        "    #selecting only needed features\n",
        "    features = {k:all_features[k] for k in photos}\n",
        "    return features\n",
        "\n",
        "\n",
        "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "\n",
        "#train = loading_data(filename)\n",
        "train_imgs = load_photos(filename)\n",
        "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
        "train_features = load_features(train_imgs)\n",
        "\n",
        "#converting dictionary to clean list of descriptions\n",
        "def dict_to_list(descriptions):\n",
        "    all_desc = []\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "#creating tokenizer class\n",
        "#this will vectorise text corpus\n",
        "#each integer will represent token in dictionary\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer\n",
        "\n",
        "# give each word an index, and store that into tokenizer.p pickle file\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "\n",
        "#calculate maximum length of descriptions\n",
        "def max_length(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    return max(len(d.split()) for d in desc_list)\n",
        "\n",
        "max_length = max_length(train_descriptions)\n",
        "print(max_length)\n",
        "\n",
        "#create input-output sequence pairs from the image description.\n",
        "\n",
        "#data generator, used by model.fit()\n",
        "def data_generator(descriptions, features, tokenizer, max_length):\n",
        "    def generator():\n",
        "        while True:\n",
        "            for key, description_list in descriptions.items():\n",
        "                feature = features[key][0]\n",
        "                input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
        "                for i in range(len(input_image)):\n",
        "                    yield {'input_1': input_image[i], 'input_2': input_sequence[i]}, output_word[i]\n",
        "\n",
        "    # Define the output signature for the generator\n",
        "    output_signature = (\n",
        "        {\n",
        "            'input_1': tf.TensorSpec(shape=(2048,), dtype=tf.float32),\n",
        "            'input_2': tf.TensorSpec(shape=(max_length,), dtype=tf.int32)\n",
        "        },\n",
        "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)\n",
        "    )\n",
        "\n",
        "    # Create the dataset\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=output_signature\n",
        "    )\n",
        "\n",
        "    return dataset.batch(32)\n",
        "\n",
        "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    # walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "        # encode the sequence\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        # split one sequence into multiple X,y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "            # split into input and output pair\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # pad input sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode output sequence\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # store\n",
        "            X1.append(feature)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "#You can check the shape of the input and output for your model\n",
        "dataset = data_generator(train_descriptions, features, tokenizer, max_length)\n",
        "for (a, b) in dataset.take(1):\n",
        "    print(a['input_1'].shape, a['input_2'].shape, b.shape)\n",
        "    break\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "\n",
        "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
        "    inputs1 = Input(shape=(2048,), name='input_1')\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    # LSTM sequence model\n",
        "    inputs2 = Input(shape=(max_length,), name='input_2')\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    # Merging both models\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "# train our model\n",
        "print('Dataset: ', len(train_imgs))\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "print('Photos: train=', len(train_features))\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Description Length: ', max_length)\n",
        "\n",
        "model = define_model(vocab_size, max_length)\n",
        "epochs = 10\n",
        "\n",
        "def get_steps_per_epoch(train_descriptions):\n",
        "    total_sequences = 0\n",
        "    for img_captions in train_descriptions.values():\n",
        "        for caption in img_captions:\n",
        "            words = caption.split()\n",
        "            total_sequences += len(words) - 1\n",
        "    # Ensure at least 1 step, even if sequences < batch_size\n",
        "    return max(1, total_sequences // 32)\n",
        "\n",
        "# Update training loop\n",
        "steps = get_steps_per_epoch(train_descriptions)\n",
        "\n",
        "# making a directory models to save our models\n",
        "os.mkdir(\"models2\")\n",
        "for i in range(epochs):\n",
        "    dataset = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
        "    model.fit(dataset, epochs=4, steps_per_epoch=steps, verbose=1)\n",
        "    model.save(\"models2/model_\" + str(i) + \".h5\")"
      ]
    }
  ]
}